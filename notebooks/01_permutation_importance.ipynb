{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 01: Permutation Importance\n",
        "\n",
        "## Breaking the Connection to Measure Impact\n",
        "\n",
        "In the previous notebook, we learned that shuffling rows shouldn't change predictions. Now we'll use a similar idea—but more strategically—to measure **feature importance**.\n",
        "\n",
        "Permutation importance answers a simple question: *\"What happens to model performance when we break the association between a feature and the target?\"*\n",
        "\n",
        "---\n",
        "\n",
        "## The Core Idea\n",
        "\n",
        "### Intuition\n",
        "\n",
        "Imagine you're predicting house prices, and one of your features is \"number of bedrooms.\" This feature is clearly important—houses with more bedrooms tend to cost more.\n",
        "\n",
        "Now, what if we randomly shuffle the \"bedrooms\" column? We break the connection between bedrooms and price. If the model's performance drops significantly, bedrooms were important. If performance barely changes, bedrooms didn't matter much.\n",
        "\n",
        "### The Algorithm\n",
        "\n",
        "Permutation importance works as follows:\n",
        "\n",
        "1. **Train a model** on the original data and record baseline performance (e.g., RMSE, R²)\n",
        "2. **For each feature**:\n",
        "   - Create a copy of the test set\n",
        "   - Randomly permute (shuffle) that feature's values\n",
        "   - Make predictions with the permuted feature\n",
        "   - Compute the performance metric\n",
        "   - **Importance = Baseline Performance - Permuted Performance**\n",
        "3. **Rank features** by importance (larger drop = more important)\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For a feature $j$, permutation importance is:\n",
        "\n",
        "$$\\text{Importance}_j = \\text{Score}(y, \\hat{y}) - \\text{Score}(y, \\hat{y}_{\\text{perm}(j)})$$\n",
        "\n",
        "where:\n",
        "- $\\text{Score}$ is a metric (higher is better, like R²) or negative error (like -RMSE)\n",
        "- $\\hat{y}$ are predictions on original test set\n",
        "- $\\hat{y}_{\\text{perm}(j)}$ are predictions when feature $j$ is permuted\n",
        "\n",
        "**Important**: For error metrics (RMSE, MAE), we typically use the **negative** so that higher values mean more importance:\n",
        "\n",
        "$$\\text{Importance}_j = -\\text{RMSE}(y, \\hat{y}_{\\text{perm}(j)}) - (-\\text{RMSE}(y, \\hat{y})) = \\text{RMSE}(y, \\hat{y}) - \\text{RMSE}(y, \\hat{y}_{\\text{perm}(j)})$$\n",
        "\n",
        "---\n",
        "\n",
        "## Why Permutation Importance?\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. **Model-agnostic**: Works with any model (linear, trees, neural networks)\n",
        "2. **Intuitive**: Easy to explain to non-technical stakeholders\n",
        "3. **No retraining**: Fast to compute (just permute and predict)\n",
        "4. **Handles interactions**: Captures feature importance in the context of the full model\n",
        "\n",
        "### Limitations\n",
        "\n",
        "1. **Correlated features**: If two features are highly correlated, permuting one might not hurt much because the other carries the signal\n",
        "2. **Computational cost**: Requires multiple predictions (one per feature, often with repeats)\n",
        "3. **Random variation**: Results can vary slightly between runs (use `n_repeats` to average)\n",
        "\n",
        "---\n",
        "\n",
        "## Permutation Importance vs. Other Methods\n",
        "\n",
        "### vs. Coefficient Magnitude (Linear Models)\n",
        "\n",
        "In linear models, coefficients tell us feature importance, but:\n",
        "- Coefficients assume features are independent (often not true)\n",
        "- Coefficients are sensitive to feature scaling\n",
        "- Permutation importance works even when features are correlated\n",
        "\n",
        "### vs. Feature Importance (Tree Models)\n",
        "\n",
        "Tree models (Random Forest, XGBoost) have built-in feature importance:\n",
        "- Based on how often features are used for splitting\n",
        "- Can be biased toward high-cardinality features\n",
        "- Permutation importance is more reliable for ranking\n",
        "\n",
        "### vs. Label Permutation Test\n",
        "\n",
        "**Important distinction**: Permutation importance permutes **features**, not labels.\n",
        "\n",
        "- **Feature permutation**: Breaks feature-target association → measures feature importance\n",
        "- **Label permutation**: Breaks all associations → tests if model learned anything (null hypothesis test)\n",
        "\n",
        "---\n",
        "\n",
        "## What We'll Do in This Notebook\n",
        "\n",
        "1. **Train a Ridge regression model** on the diabetes dataset\n",
        "2. **Compute permutation importance** using scikit-learn's built-in function\n",
        "3. **Visualize results** with a bar plot\n",
        "4. **Manual verification**: Manually permute one feature and observe the impact\n",
        "\n",
        "Let's begin!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from src.utils import set_seed\n",
        "from src.metrics import regression_report\n",
        "from src.viz import barh\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "set_seed(42)\n",
        "print(\"✓ Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "data = load_diabetes(as_frame=True)\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"\\nFeature names: {list(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Train a Model with Pipeline\n",
        "\n",
        "We'll use a **Pipeline** with StandardScaler + Ridge. Standardization is important for Ridge regression because it penalizes coefficients equally. Without scaling, features with larger scales would be penalized more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Train a Ridge or LinearRegression model on the diabetes data.\n",
        "# Hints: use a Pipeline with StandardScaler for stability\n",
        "# Acceptance:\n",
        "# - Print test RMSE and R2\n",
        "# - Pipeline is fitted successfully\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compute Permutation Importance\n",
        "\n",
        "Scikit-learn provides `permutation_importance()` which permutes each feature multiple times and returns mean and std of importance scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Compute permutation importance on the test set.\n",
        "# Hints: from sklearn.inspection import permutation_importance; n_repeats=10\n",
        "# Acceptance:\n",
        "# - Bar plot of importances; short interpretation paragraph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Manual Permutation Check\n",
        "\n",
        "As a sanity check, manually permute one feature and observe the impact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Kaggle-style quick check: manually permute one obviously strong feature and recompute RMSE.\n",
        "# Hints: copy X_test; shuffle one column with np.random.permutation\n",
        "# Acceptance:\n",
        "# - Show metric delta; if small, feature likely weak for this model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Permutation importance is a powerful, model-agnostic way to measure feature importance. It works by breaking the connection between a feature and the target through random permutation, then measuring the impact on model performance.\n",
        "\n",
        "**Next**: Notebook 02 will explore regularization (Ridge and Lasso) to understand how we can control model complexity.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
