{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Regularization (Ridge & Lasso)\n",
    "\n",
    "## The Art of Restraint\n",
    "\n",
    "Overfitting is the enemy of generalization. Regularization tames it by penalizing complexity. Ridge shrinks coefficients smoothly. Lasso can zero them out entirely, performing automatic feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Regularize?\n",
    "\n",
    "### The Overfitting Problem\n",
    "\n",
    "When a model has too many parameters relative to the data, it can memorize the training set instead of learning generalizable patterns.\n",
    "\n",
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "Regularization introduces a controlled amount of **bias** to reduce **variance**.\n",
    "\n",
    "## Ridge vs Lasso\n",
    "\n",
    "- **Ridge (L2)**: Penalizes sum of squared coefficients. Shrinks all coefficients toward zero.\n",
    "- **Lasso (L1)**: Penalizes sum of absolute coefficients. Can set coefficients to exactly zero.\n",
    "\n",
    "We use cross-validation to pick the regularization strength (alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path().resolve().parent if Path().resolve().name == 'notebooks' else Path().resolve()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.utils import set_seed\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"\u2713 Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "data = load_diabetes(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit RidgeCV with Cross-Validation\n",
    "\n",
    "RidgeCV automatically finds the best alpha using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TODO (you code this) ===\n",
    "# Pipeline: StandardScaler + RidgeCV over alphas logspace(1e-3..1e3)\n",
    "# Hints:\n",
    "#   - Create Pipeline with StandardScaler and RidgeCV\n",
    "#   - Use alphas=np.logspace(-3, 3, 100) for RidgeCV\n",
    "#   - Fit on X_train, y_train\n",
    "#   - Make predictions and compute RMSE, R\u00b2\n",
    "# Acceptance: Print best alpha, test RMSE, R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fit LassoCV with Cross-Validation\n",
    "\n",
    "LassoCV automatically finds the best alpha and performs feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TODO (you code this) ===\n",
    "# Pipeline: StandardScaler + LassoCV. Compare metrics with Ridge.\n",
    "# Hints:\n",
    "#   - Create Pipeline with StandardScaler and LassoCV\n",
    "#   - Use alphas=np.logspace(-3, 1, 100) for LassoCV (smaller range)\n",
    "#   - Fit and evaluate\n",
    "#   - Create comparison table\n",
    "# Acceptance: Table with RMSE, MAE, R2 for both; 2-sentence comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Coefficient Comparison\n",
    "\n",
    "Compare how Ridge and Lasso treat coefficients differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TODO (you code this) ===\n",
    "# Plot coefficient magnitudes side by side for Ridge vs Lasso.\n",
    "# Hints:\n",
    "#   - Extract coefficients from both models\n",
    "#   - Create side-by-side bar plot\n",
    "#   - Note which coefficients are zero in Lasso\n",
    "#   - Save to images/02_ridge_lasso_coefficients.png\n",
    "# Acceptance: Figure with clear legend; note which coefficients go to zero with Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've learned about regularization:\n",
    "\n",
    "- **Ridge** shrinks coefficients smoothly, handles multicollinearity\n",
    "- **Lasso** can zero out coefficients, performs automatic feature selection\n",
    "- Cross-validation helps us find the optimal regularization strength\n",
    "\n",
    "**Next**: Notebook 03 will explore multicollinearity and PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}